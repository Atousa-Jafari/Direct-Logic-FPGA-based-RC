{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a844a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: PyTorchESN Class Definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PyTorchESN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 reservoir_dim,\n",
    "                 output_dim,\n",
    "                 spectral_radius=0.9,\n",
    "                 W_res_density=0.1,\n",
    "                 input_scaling=1.0,\n",
    "                 bias_scaling=0.2,\n",
    "                 leak_rate=1.0,\n",
    "                 activation_func=torch.tanh,\n",
    "                 Win_data=None,\n",
    "                 W_res_data=None,\n",
    "                 bias_res_data=None,\n",
    "                 W_out_data=None,\n",
    "                 bias_out_data=None,\n",
    "                 readout_uses_input=False):\n",
    "\n",
    "        super(PyTorchESN, self).__init__()\n",
    "\n",
    "        # ---- Store Hyperparameters and Configuration ----\n",
    "        self.input_dim = input_dim\n",
    "        self.reservoir_dim = reservoir_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.W_res_density = W_res_density\n",
    "        self.input_scaling = input_scaling\n",
    "        self.bias_scaling = bias_scaling\n",
    "        self.leak_rate = leak_rate\n",
    "        self.activation_func = activation_func\n",
    "        self.readout_uses_input = readout_uses_input\n",
    "\n",
    "        # This method handles all weight initializations\n",
    "        self.load_weights_from_data(Win_data, W_res_data, bias_res_data, W_out_data, bias_out_data)\n",
    "\n",
    "        self.current_reservoir_state = None\n",
    "        self.is_initial_state = True\n",
    "\n",
    "    def _initialize_W_res(self):\n",
    "        W_res_ = torch.randn(self.reservoir_dim, self.reservoir_dim)\n",
    "        num_zero_elements = int((1 - self.W_res_density) * self.reservoir_dim**2)\n",
    "        if 0 < num_zero_elements < self.reservoir_dim**2:\n",
    "            zero_indices = torch.randperm(self.reservoir_dim**2)[:num_zero_elements]\n",
    "            W_res_.view(-1)[zero_indices] = 0\n",
    "        \n",
    "        if self.reservoir_dim > 0:\n",
    "            try:\n",
    "                eigenvalues = torch.linalg.eigvals(W_res_)\n",
    "                current_spectral_radius = torch.max(torch.abs(eigenvalues))\n",
    "                if current_spectral_radius > 1e-9:\n",
    "                    W_res_scaled = W_res_ * (self.spectral_radius / current_spectral_radius)\n",
    "                else:\n",
    "                    W_res_scaled = W_res_\n",
    "            except Exception:\n",
    "                W_res_scaled = W_res_\n",
    "        else:\n",
    "            W_res_scaled = W_res_\n",
    "        return W_res_scaled\n",
    "\n",
    "    def _update_reservoir_state(self, current_input, previous_reservoir_state):\n",
    "        \"\"\"\n",
    "        Computes one step of the reservoir state update using the correct matrix multiplication order.\n",
    "        r(t) = (1-lr)*r(t-1) + lr*activation( W_in @ x(t) + W_res @ r(t-1) + bias_res )\n",
    "        \"\"\"\n",
    "        input_contrib = torch.matmul(self.Win, current_input.T).T\n",
    "        reservoir_contrib = torch.matmul(self.W_res, previous_reservoir_state.T).T\n",
    "        pre_activation = input_contrib + reservoir_contrib + self.bias_res\n",
    "        activated_state = self.activation_func(pre_activation)\n",
    "        new_reservoir_state = (1 - self.leak_rate) * previous_reservoir_state + self.leak_rate * activated_state\n",
    "        return new_reservoir_state\n",
    "    \n",
    "    def forward(self, input_sequence, initial_reservoir_state=None):\n",
    "        batch_size, sequence_length, _ = input_sequence.shape\n",
    "        device = input_sequence.device\n",
    "        \n",
    "        if initial_reservoir_state is None:\n",
    "            current_reservoir_state = torch.zeros(batch_size, self.reservoir_dim, device=device)\n",
    "        else:\n",
    "            current_reservoir_state = initial_reservoir_state.to(device)\n",
    "\n",
    "        # Since seq_length is 1 for this app, this loop runs only once.\n",
    "        for t in range(sequence_length):\n",
    "            current_input_t = input_sequence[:, t, :]\n",
    "            current_reservoir_state = self._update_reservoir_state(current_input_t, current_reservoir_state)\n",
    "            \n",
    "        readout_input = current_reservoir_state\n",
    "        if self.readout_uses_input:\n",
    "            readout_input = torch.cat((current_reservoir_state, current_input_t), dim=1)\n",
    "        \n",
    "        final_output = self.W_out_layer(readout_input)\n",
    "        \n",
    "        return final_output, current_reservoir_state\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.current_reservoir_state = None\n",
    "        self.is_initial_state = True\n",
    "        \n",
    "    def load_weights_from_data(self, Win_data, W_res_data, bias_res_data, W_out_data, bias_out_data):\n",
    "        if Win_data is not None:\n",
    "            self.Win = nn.Parameter(torch.tensor(Win_data, dtype=torch.float32), requires_grad=False)\n",
    "        else:\n",
    "            Win_ = torch.rand(self.reservoir_dim, self.input_dim) * 2 - 1\n",
    "            self.Win = nn.Parameter(Win_ * self.input_scaling, requires_grad=False)\n",
    "            \n",
    "        if bias_res_data is not None:\n",
    "            self.bias_res = nn.Parameter(torch.tensor(bias_res_data.flatten(), dtype=torch.float32), requires_grad=False)\n",
    "        else:\n",
    "            bias_res_ = torch.rand(self.reservoir_dim) * 2 - 1\n",
    "            self.bias_res = nn.Parameter(bias_res_ * self.bias_scaling, requires_grad=False)\n",
    "            \n",
    "        if W_res_data is not None:\n",
    "            self.W_res = nn.Parameter(torch.tensor(W_res_data, dtype=torch.float32), requires_grad=False)\n",
    "        else:\n",
    "            self.W_res = nn.Parameter(self._initialize_W_res(), requires_grad=False)\n",
    "\n",
    "        readout_input_dim = self.reservoir_dim + (self.input_dim if self.readout_uses_input else 0)\n",
    "        self.W_out_layer = nn.Linear(readout_input_dim, self.output_dim, bias=True)\n",
    "\n",
    "        if W_out_data is not None:\n",
    "            W_out_numpy = np.asarray(W_out_data)\n",
    "            expected_shape = (self.output_dim, readout_input_dim)\n",
    "            if W_out_numpy.shape == expected_shape:\n",
    "                W_out_to_load = W_out_numpy\n",
    "            elif W_out_numpy.shape == (expected_shape[1], expected_shape[0]):\n",
    "                W_out_to_load = W_out_numpy.T\n",
    "            else:\n",
    "                raise ValueError(f\"Shape mismatch for W_out_data. Expected {expected_shape} or transpose, got {W_out_numpy.shape}\")\n",
    "            self.W_out_layer.weight = nn.Parameter(torch.tensor(W_out_to_load, dtype=torch.float32), requires_grad=False)\n",
    "        else:\n",
    "            nn.init.zeros_(self.W_out_layer.weight)\n",
    "\n",
    "        if bias_out_data is not None:\n",
    "            bias_out_numpy = np.asarray(bias_out_data).flatten()\n",
    "            if bias_out_numpy.shape[0] != self.output_dim:\n",
    "                raise ValueError(f\"Shape mismatch for bias_out_data. Expected ({self.output_dim},), got {bias_out_numpy.shape}\")\n",
    "            self.W_out_layer.bias = nn.Parameter(torch.tensor(bias_out_numpy, dtype=torch.float32), requires_grad=False)\n",
    "        else:\n",
    "            if self.W_out_layer.bias is not None:\n",
    "                nn.init.zeros_(self.W_out_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5735cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 11:05:28.576548: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-20 11:05:28.583708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753002328.592494  641111 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753002328.595033  641111 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753002328.601656  641111 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753002328.601667  641111 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753002328.601668  641111 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753002328.601668  641111 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-20 11:05:28.604181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Data ---\n",
      "Flattened training data shape: (500, 140)\n",
      "One-hot training labels shape: (500, 5)\n",
      "\n",
      "--- Training ReservoirPy ESN Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Model-0: 500it [00:00, 15412.53it/s]          \n",
      "Running Model-0: 100%|██████████| 1/1 [00:00<00:00, 28.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node Ridge-0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReservoirPy ESN training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Model-0: 4500it [00:00, 16273.18it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReservoirPy Original Accuracy: 93.04%\n",
      "\n",
      "--- Extracting Effective Weights from ReservoirPy Model ---\n",
      "\n",
      "--- Instantiating PyTorchESN with Loaded Weights ---\n",
      "PyTorchESN instantiated and ready for verification.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Train ReservoirPy and Instantiate PyTorch ESN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from aeon.datasets import load_classification\n",
    "from reservoirpy.nodes import Reservoir, Ridge\n",
    "\n",
    "# ========== 1. Load and Prepare Data ==========\n",
    "print(\"--- Loading and Preparing Data ---\")\n",
    "X_train_orig, y_train_orig = load_classification('ECG5000', split='train')\n",
    "X_test_orig, y_test_orig = load_classification('ECG5000', split='test')\n",
    "\n",
    "# Squeeze the middle dimension (n_dims=1)\n",
    "X_train = X_train_orig.squeeze()\n",
    "X_test = X_test_orig.squeeze()\n",
    "\n",
    "# Flatten each time-series sample into a single feature vector\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Encode labels and convert to one-hot format\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_orig)\n",
    "y_test = label_encoder.transform(y_test_orig)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "y_test_labels = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(f\"Flattened training data shape: {X_train_reshaped.shape}\")\n",
    "print(f\"One-hot training labels shape: {y_train_cat.shape}\")\n",
    "\n",
    "# ========== 2. Define and Train ReservoirPy ESN Model ==========\n",
    "print(\"\\n--- Training ReservoirPy ESN Model ---\")\n",
    "INPUT_DIM = X_train_reshaped.shape[1]\n",
    "RESERVOIR_DIM = 200\n",
    "OUTPUT_DIM = y_train_cat.shape[1]\n",
    "\n",
    "reservoir = Reservoir(units=RESERVOIR_DIM, input_dim=INPUT_DIM, sr=0.4, input_connectivity=0.8,\n",
    "                      rc_connectivity=0.1, input_scaling=0.3, lr=1, seed=43)\n",
    "readout = Ridge(output_dim=OUTPUT_DIM, ridge=1e-1)\n",
    "esn_model = reservoir >> readout\n",
    "\n",
    "esn_model.fit(X_train_reshaped, y_train_cat)\n",
    "print(\"ReservoirPy ESN training complete.\")\n",
    "\n",
    "# ========== 3. Calculate ReservoirPy Model Accuracy ==========\n",
    "y_pred_rp = esn_model.run(X_test_reshaped)\n",
    "y_pred_labels_rp = np.argmax(y_pred_rp, axis=1)\n",
    "acc_rp = np.mean(y_pred_labels_rp == y_test_labels)\n",
    "print(f\"ReservoirPy Original Accuracy: {acc_rp*100:.2f}%\")\n",
    "\n",
    "# ========== 4. Get Effective Weights from ReservoirPy ==========\n",
    "print(\"\\n--- Extracting Effective Weights from ReservoirPy Model ---\")\n",
    "Win_eff = (reservoir.Win.toarray())\n",
    "W_res_eff = reservoir.W.toarray()\n",
    "bias_res_eff = (reservoir.bias.toarray().flatten())\n",
    "W_out_eff = readout.Wout\n",
    "bias_out_eff = readout.bias\n",
    "\n",
    "# ========== 5. Instantiate the PyTorchESN Model ==========\n",
    "print(\"\\n--- Instantiating PyTorchESN with Loaded Weights ---\")\n",
    "pytorch_esn_opt1 = PyTorchESN(\n",
    "    input_dim=INPUT_DIM,\n",
    "    reservoir_dim=RESERVOIR_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    Win_data=Win_eff,\n",
    "    W_res_data=W_res_eff,\n",
    "    bias_res_data=bias_res_eff,\n",
    "    W_out_data=W_out_eff,\n",
    "    bias_out_data=bias_out_eff,\n",
    "    leak_rate=reservoir.lr,\n",
    "    spectral_radius=reservoir.sr,\n",
    "    readout_uses_input=False\n",
    ")\n",
    "pytorch_esn_opt1.eval()\n",
    "print(\"PyTorchESN instantiated and ready for verification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2eb3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch test input shape: torch.Size([4500, 1, 140])\n",
      "\n",
      "--- Verification ---\n",
      "ReservoirPy Original Accuracy:      93.04%\n",
      "PyTorch Re-implementation Accuracy: 92.80%\n",
      "\n",
      "SUCCESS: Accuracies are very close. Proceeding to ONNX export.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Verify PyTorch Model\n",
    "# Reshape the flattened test data to (n_samples, 1, n_features) for the PyTorch model\n",
    "import numpy\n",
    "\n",
    "X_test_torch = torch.tensor(X_test_reshaped, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"PyTorch test input shape: {X_test_torch.shape}\")\n",
    "\n",
    "# Get PyTorch ESN Predictions\n",
    "pytorch_esn_opt1.eval()\n",
    "pytorch_esn_opt1.reset_state()\n",
    "with torch.no_grad():\n",
    "    y_pred_torch, _ = pytorch_esn_opt1(X_test_torch)\n",
    "\n",
    "y_pred_labels_torch = np.argmax(y_pred_torch.numpy(), axis=1)\n",
    "acc_torch = np.mean(y_pred_labels_torch == y_test_labels)\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"ReservoirPy Original Accuracy:      {acc_rp*100:.2f}%\")\n",
    "print(f\"PyTorch Re-implementation Accuracy: {acc_torch*100:.2f}%\")\n",
    "\n",
    "if np.isclose(acc_rp, acc_torch, atol=1e-2):\n",
    "    print(\"\\nSUCCESS: Accuracies are very close. Proceeding to ONNX export.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Accuracies differ significantly. Please review the implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e003c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "General_Batch_Size = 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8aa397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exporting to ONNX: esn_fp32.onnx ---\n",
      "ONNX export successful.\n",
      "FP32 ONNX model saved to: esn_fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Export to ONNX\n",
    "ONNX_MODEL_PATH = \"esn_fp32.onnx\"\n",
    "DUMMY_BATCH_SIZE = General_Batch_Size\n",
    "DUMMY_SEQUENCE_LENGTH = 1 # Fixed sequence length of 1\n",
    "\n",
    "pytorch_esn_opt1.eval()\n",
    "pytorch_esn_opt1.to('cpu')\n",
    "pytorch_esn_opt1.reset_state()\n",
    "\n",
    "dummy_input_sequence = torch.randn(\n",
    "    DUMMY_BATCH_SIZE, DUMMY_SEQUENCE_LENGTH, INPUT_DIM, device='cpu', dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Exporting to ONNX: {ONNX_MODEL_PATH} ---\")\n",
    "torch.onnx.export(\n",
    "    pytorch_esn_opt1,\n",
    "    (dummy_input_sequence,),\n",
    "    ONNX_MODEL_PATH,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_sequence'],\n",
    "    output_names=['final_output', 'final_reservoir_state'],\n",
    "    dynamic_axes={\n",
    "        'input_sequence': {0: 'batch_size'},\n",
    "        'final_output': {0: 'batch_size'},\n",
    "        'final_reservoir_state': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "print(\"ONNX export successful.\")\n",
    "print(f\"FP32 ONNX model saved to: {ONNX_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2d6fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing dynamic quantization for ONNX model...\n",
      "Ignore MatMul due to non constant B: /[/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/MatMul_1]\n",
      "Quantized INT8 ONNX model saved to: esn_uint8.onnx\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Post-Training Quantization to create INT8 ONNX model\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "ONNX_QUANT_PATH = \"esn_uint8.onnx\"\n",
    "\n",
    "print(\"\\nPerforming dynamic quantization for ONNX model...\")\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=ONNX_MODEL_PATH,\n",
    "    model_output=ONNX_QUANT_PATH,\n",
    "    weight_type=QuantType.QUInt8 # Use UINT8 for ARM, as it's often well-supported\n",
    ")\n",
    "\n",
    "print(f\"Quantized INT8 ONNX model saved to: {ONNX_QUANT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c679ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.6908442 ,  0.71141435, -2.1140915 , ...,  1.0359968 ,\n",
       "         1.4922866 , -1.9050734 ],\n",
       "       [-1.3481323 , -3.9960376 , -4.2267496 , ...,  0.26753217,\n",
       "         1.0711484 , -1.164009  ],\n",
       "       [ 1.0242946 , -0.59031419, -1.9169491 , ...,  1.8163009 ,\n",
       "         1.4739633 ,  1.3897666 ],\n",
       "       ...,\n",
       "       [-1.3517791 , -2.2090058 , -2.5202247 , ..., -2.2600228 ,\n",
       "        -1.577823  , -0.68453092],\n",
       "       [-1.1244318 , -1.9050388 , -2.1927069 , ..., -0.4433066 ,\n",
       "        -0.55976901,  0.10856792],\n",
       "       [ 0.72881283,  0.19259731, -0.73388442, ...,  1.5389752 ,\n",
       "         1.713781  ,  1.3093816 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_reshaped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61992ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03bb1a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test data to files...\n",
      "Successfully saved test data to 'X_test_reshaped.npy' and 'y_test_labels.npy'.\n",
      "You can now transfer these files to your Zynq board.\n"
     ]
    }
   ],
   "source": [
    "# --- Save the necessary arrays to .npy files ---\n",
    "X_TEST_FILE = 'X_test_reshaped.npy'\n",
    "Y_TEST_FILE = 'y_test_labels.npy'\n",
    "\n",
    "print(f\"Saving test data to files...\")\n",
    "np.save(X_TEST_FILE, X_test_reshaped)\n",
    "np.save(Y_TEST_FILE, y_test_labels)\n",
    "\n",
    "print(f\"Successfully saved test data to '{X_TEST_FILE}' and '{Y_TEST_FILE}'.\")\n",
    "print(\"You can now transfer these files to your Zynq board.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a696c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6-7: Final benchmarking script to run ON THE ZYNQ\n",
    "\n",
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "# import timeit\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Add this class to your Python script on the Zynq\n",
    "\n",
    "# import threading\n",
    "# import time\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# class ZynqPowerMonitor:\n",
    "#     \"\"\"\n",
    "#     A thread-based power monitor for Zynq UltraScale+ MPSoCs by reading sysfs files.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, hwmon_path=None, interval_sec=0.1):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             hwmon_path (str, optional): The full path to the hwmon directory. \n",
    "#                                         If None, it will try to find it automatically.\n",
    "#             interval_sec (float): The polling interval in seconds.\n",
    "#         \"\"\"\n",
    "#         self.interval = interval_sec\n",
    "#         self.power_readings_watts = []\n",
    "        \n",
    "#         self._stop_event = threading.Event()\n",
    "#         self._monitor_thread = None\n",
    "\n",
    "#         if hwmon_path:\n",
    "#             self.power_file_path = os.path.join(hwmon_path, 'power1_input')\n",
    "#         else:\n",
    "#             # Attempt to find the power file automatically\n",
    "#             self.power_file_path = self._find_power_file()\n",
    "\n",
    "#         if not os.path.exists(self.power_file_path):\n",
    "#             raise FileNotFoundError(f\"Power monitor file not found at '{self.power_file_path}'. \"\n",
    "#                                     \"Please find the correct path using 'find /sys/bus/i2c/devices/ -name \\\"power1_input\\\"'\")\n",
    "\n",
    "#     def _find_power_file(self):\n",
    "#         \"\"\"Tries to locate the 'power1_input' file automatically.\"\"\"\n",
    "#         for root, dirs, files in os.walk('/sys/bus/i2c/devices/'):\n",
    "#             if 'power1_input' in files:\n",
    "#                 # This logic could be improved to pick a specific sensor if there are multiple.\n",
    "#                 # For most boards, the first one found is for the main PS rails.\n",
    "#                 return os.path.join(root, 'power1_input')\n",
    "#         return None\n",
    "\n",
    "#     def _get_power_reading_watts(self):\n",
    "#         \"\"\"Reads the sysfs file and converts from microwatts to Watts.\"\"\"\n",
    "#         try:\n",
    "#             with open(self.power_file_path, 'r') as f:\n",
    "#                 power_microwatts = int(f.read())\n",
    "#             # Convert from microwatts to Watts\n",
    "#             return power_microwatts / 1_000_000.0\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: Could not read power file. Error: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     def _monitor_power_loop(self):\n",
    "#         \"\"\"The main loop for the monitoring thread.\"\"\"\n",
    "#         while not self._stop_event.is_set():\n",
    "#             power = self._get_power_reading_watts()\n",
    "#             if power is not None:\n",
    "#                 self.power_readings_watts.append(power)\n",
    "#             time.sleep(self.interval)\n",
    "\n",
    "#     def start(self):\n",
    "#         \"\"\"Starts the power monitoring background thread.\"\"\"\n",
    "#         print(\"Starting Zynq power monitor...\")\n",
    "#         self.power_readings_watts = []\n",
    "#         self._stop_event.clear()\n",
    "#         self.start_time = time.time()\n",
    "#         self._monitor_thread = threading.Thread(target=self._monitor_power_loop)\n",
    "#         self._monitor_thread.start()\n",
    "\n",
    "#     def stop(self):\n",
    "#         \"\"\"Stops the monitoring thread and returns the calculated metrics.\"\"\"\n",
    "#         self._stop_event.set()\n",
    "#         if self._monitor_thread is not None:\n",
    "#             self._monitor_thread.join()\n",
    "#         self.end_time = time.time()\n",
    "#         print(\"Power monitor stopped.\")\n",
    "        \n",
    "#         duration_sec = self.end_time - self.start_time\n",
    "#         avg_power = np.mean(self.power_readings_watts) if self.power_readings_watts else 0.0\n",
    "#         energy_j = avg_power * duration_sec\n",
    "#         latency_ms = duration_sec * 1000\n",
    "        \n",
    "#         return latency_ms, avg_power, energy_j\n",
    "\n",
    "# # --- Helper function for ONNX Runtime inference ---\n",
    "# def run_ort_inference(model_path, test_data, execution_provider):\n",
    "#     # Set session options and specify the execution provider\n",
    "#     sess_options = ort.SessionOptions()\n",
    "#     ort_session = ort.Session(model_path, sess_options, providers=[execution_provider])\n",
    "    \n",
    "#     input_name = ort_session.get_inputs()[0].name\n",
    "    \n",
    "#     # Run inference for all test data\n",
    "#     predictions = ort_session.run(None, {input_name: test_data})[0]\n",
    "#     return predictions\n",
    "\n",
    "# # --- Main Execution on Zynq ---\n",
    "# if __name__ == '__main__':\n",
    "#     # Load your test data on the Zynq\n",
    "#     X_test_reshaped = np.load('X_test_reshaped.npy').astype(np.float32)\n",
    "#     # Reshape to (batch_size, 1, features)\n",
    "#     test_input_data_np = X_test_reshaped.reshape(-1, 1, X_test_reshaped.shape[1])\n",
    "#     y_test_labels = np.load('y_test_labels.npy')\n",
    "    \n",
    "#     zynq_monitor = ZynqPowerMonitor()\n",
    "\n",
    "#     models_to_benchmark = {\n",
    "#         \"ONNX FP32\": \"esn_fp32.onnx\",\n",
    "#         \"ONNX INT8\": \"esn_uint8.onnx\"\n",
    "#     }\n",
    "    \n",
    "#     print(\"\\n--- Zynq Performance & Accuracy Report ---\")\n",
    "#     print(\"=\"*100)\n",
    "#     header = f\"{'Model':<15} | {'Accuracy (%)':<15} | {'Latency (s)':<15} | {'Throughput (inf/s)':<20} | {'Avg Power (W)':<15} | {'Energy (J)':<12}\"\n",
    "#     print(header)\n",
    "#     print(\"-\" * 100)\n",
    "\n",
    "#     for name, path in models_to_benchmark.items():\n",
    "#         print(f\"Benchmarking {name}...\")\n",
    "        \n",
    "#         # Use 'CPUExecutionProvider', which is optimized for ARM NEON\n",
    "#         exec_provider = 'CPUExecutionProvider'\n",
    "\n",
    "#         # Time the full inference run\n",
    "#         def workload():\n",
    "#             return run_ort_inference(path, test_input_data_np, exec_provider)\n",
    "        \n",
    "#         # Warm-up run\n",
    "#         _ = workload()\n",
    "\n",
    "#         # Measure power and energy for a single, full-batch run\n",
    "#         zynq_monitor.start()\n",
    "#         predictions = workload()\n",
    "#         latency, avg_power, energy = zynq_monitor.stop()\n",
    "\n",
    "#         latency_sec = latency / 1000.0\n",
    "#         throughput = len(X_test_reshaped) / latency_sec\n",
    "        \n",
    "#         # Calculate accuracy\n",
    "#         pred_labels = np.argmax(predictions, axis=1)\n",
    "#         acc = accuracy_score(y_test_labels, pred_labels)\n",
    "        \n",
    "#         # Print results\n",
    "#         print(f\"{name:<15} | {acc*100:<15.2f} | {latency_sec:<15.4f} | {throughput:<20.2f} | {avg_power:<15.4f} | {energy:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83fac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a09b5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnxruntime as ort\n",
    "# onnxruntime__version__ = ort.__version__\n",
    "# print (onnxruntime__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f936aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ort.InferenceSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b6acf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --force-reinstall --ignore-installed -r reqs.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
